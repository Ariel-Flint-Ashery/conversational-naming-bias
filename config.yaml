network:
  network_type: 'complete'
  degree: 4
  alpha: 0.41
  beta: 0.54 
  erdos_p: 0.5
minority:
  minority_size_set: [0]
  committment_index: 1
params:
  temperature: 0.5
  runs: 6
  total_interactions: 5000 #only used for committed runs
  N: 24
  initial_composition: 0.95
  initial: 'None'
  convergence_time: 72
  convergence_threshold: 0.95
  rewards_set: [[-50, 100]]
  memory_size_set: [5]
  options_set: 'stereo' #[['A', 'B']]
sim:
  version: 'swap'
  continue_evolution: False
  stochastic: True
  mode: 'api' #'gpu'
model:
  model_name: "meta-llama/Llama-3.1-70B-Instruct" 
  shorthand: "llama31_70B" #"mistral_8x7B" # "llama32_3B" #"gemma2_2b" #
  API_TOKEN: #''
  sys_prompt_is_avail: True
  quantized: False